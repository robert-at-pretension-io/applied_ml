{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data from https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)\n",
    "\n",
    "header_info = [\"age\",\n",
    "              \"class_of_worker\",\n",
    "              \"industry code\",\n",
    "              \"occupation code\",\n",
    "              \"level of education\",\n",
    "              \"wage per hour\",\n",
    "              \"entrolled in education as of last week\",\n",
    "              \"marital status\",\n",
    "              \"major industry code\",\n",
    "              \"major occupation code\",\n",
    "              \"race\",\n",
    "              \"hispanic origin\",\n",
    "              \"sex\",\n",
    "              \"member of a labor union\",\n",
    "              \"reason for unemployment\",\n",
    "              \"full or part time employment status\",\n",
    "              \"captial gains\",\n",
    "              \"capital losses\",\n",
    "              \"dividends from stocks\",\n",
    "              \"tax filler status\",\n",
    "              \"region of previous residence\",\n",
    "              \"state of previous residence\",\n",
    "              \"detailed household and family status\",\n",
    "              \"detailed household summary\",\n",
    "              \"migration code\",\n",
    "              \"migration code - change in region\",\n",
    "              \"migration code - move within region\",\n",
    "              \"live in this house one year ago\",\n",
    "              \"migration - previous resident in sunbelt\",\n",
    "              \"number of persons that worked for employer\",\n",
    "              \"family members under 18\",\n",
    "              \"country of birth father\",\n",
    "              \"country of birth mother\",\n",
    "              \"country of birth\",\n",
    "              \"citizenship\",\n",
    "              \"own business or self-employed\",\n",
    "              \"fill included questionaire for veterans administration\",\n",
    "              \"veterans benefits\",\n",
    "              \"weeks worked in the year\",\n",
    "              \"year of survey\",\n",
    "              \"income less than or greater than 50,000\",\n",
    "              \"number of years of education\"]\n",
    "\n",
    "print(len(header_info))\n",
    "\n",
    "my_census = pd.read_csv(\"./census-income.data\", names=header_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_census.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%matplotlib inline\\nimport matplotlib.pyplot as plt\\nmy_census.hist(bins=10, figsize=(20,15))\\nplt.show()'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have our data... now let's look at it\n",
    "\n",
    "#my_census.describe()\n",
    "\n",
    "\"\"\"%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "my_census.hist(bins=10, figsize=(20,15))\n",
    "plt.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import hashlib\\nimport numpy as np\\n\\ndef test_set_checker(id, test_ratio, hash):\\n    return hash(np.int64(id)).digest()[-1] < 256 * test_ratio\\n\\ndef split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\\n    ids = data[id_column]\\n    in_test_set = ids.apply(lambda id_: test_set_checker(id_, test_ratio, hash))\\n    return data.loc[~in_test_set], data.loc[in_test_set]\\n\\n\\nmy_census_id = my_census.reset_index()\\n\\ntrain_set, test_set = split_train_test_by_id(my_census_id, .2, \"index\")\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to set aside data for our test set (see how well our algorithm performs!)\n",
    "\n",
    "#helper functions to split train/test data\n",
    "\n",
    "\"\"\"import hashlib\n",
    "import numpy as np\n",
    "\n",
    "def test_set_checker(id, test_ratio, hash):\n",
    "    return hash(np.int64(id)).digest()[-1] < 256 * test_ratio\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_checker(id_, test_ratio, hash))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "\n",
    "my_census_id = my_census.reset_index()\n",
    "\n",
    "train_set, test_set = split_train_test_by_id(my_census_id, .2, \"index\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#actually... just use sklearn's data splitting\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(my_census,test_size=.2,random_state=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set is 159618 many examples. Whereas the test set contains 39905 examples.\n"
     ]
    }
   ],
   "source": [
    "print(\"The train set is {} many examples. Whereas the test set contains {} examples.\".format(len(train_set),len(test_set)))\n",
    "\n",
    "#Note that we could do stratified sampling here if we knew which categories were most important\n",
    "\n",
    "census = train_set.copy()\n",
    "\n",
    "#drop anything with the word code\n",
    "#bad_feature = [x for x in header_info if \"code\" in x]\n",
    "\n",
    "#census = census.drop(axis=1, labels=bad_feature)\n",
    "#census = census.drop(axis=1,labels=['number of persons that worked for employer','family members under 18','year of survey','fill included questionaire for veterans administration'])\n",
    "\n",
    "#drop data if age < 18\n",
    "#census = census.drop(census[census['age'] <= 18].index)\n",
    "\n",
    "\n",
    "\n",
    "#census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = census.corr()\n",
    "\n",
    "correlation_matrix['wage per hour'].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# for this first project, let's try to predict wage per hour!\n",
    "\n",
    "label = \"wage per hour\"\n",
    "\n",
    "census_label = census[label]\n",
    "census = census.drop(label,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#attributes = ['dividends from stocks', 'captial gains', 'age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer helper function to fill categorical values with most frequent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"inpute missing values.\n",
    "        \n",
    "        columns of dtype object are imputed with most frequent.\n",
    "        \n",
    "        columns of other dtype are imputed with median value\"\"\"\n",
    "    def fit(self, X,y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "                              if X[c].dtype == np.dtype(object) else X[c].mean() for c in X ],\n",
    "                             index = X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got multiple values for argument 'missing_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-13975be376d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#cat_to_num_columns = encoder.fit_transform(categorical_columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimputed_census\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrameImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcensus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Not in Universe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \"\"\"for column_name in census.columns.values:\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got multiple values for argument 'missing_values'"
     ]
    }
   ],
   "source": [
    "#here we will encode all categorical values into numerical values\n",
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#categorical_columns = census.select_dtypes(exclude=[np.number])\n",
    "#numerical_columns = census.select_dtypes(include=[np.number])\n",
    "#census.hist(bins=20, figsize=(20,15))\n",
    "\n",
    "\n",
    "#cat_to_num_columns = encoder.fit_transform(categorical_columns)\n",
    "\n",
    "imputed_census = DataFrameImputer().fit_transform(census, missing_values=\"Not in Universe\")\n",
    "\n",
    "\"\"\"for column_name in census.columns.values:\n",
    "    print(\"replacing missing values :: \" + column_name)\n",
    "    #census[column_name].fillna()\n",
    "    filled_in_census = imputer.fit_transform(census)\n",
    "    #encoded = encoder.fit_transform(categorical_columns[category_column_name])\n",
    "    \n",
    "    #print(\"here's a sample: \")\n",
    "    #census[category_column_name] = encoded\n",
    "\"\"\"    \n",
    "\n",
    "\n",
    "\n",
    "class_category = pandas.get_dummies(imputed_census)\n",
    "class_category2 = pandas.get_dummies(census)\n",
    "class_category.describe()\n",
    "class_category2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class_category.info()\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "test = pca.fit(class_category.values) #.values makes the dataframe into a np array\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isolated_kernel",
   "language": "python",
   "name": "isolated_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
